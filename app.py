import streamlit as st
import os
import time
from semantic_retrieval import SemanticRAGChatbot

# Thi·∫øt l·∫≠p page config
st.set_page_config(page_title="Tr·ª£ l√Ω S·ªï tay Sinh vi√™n", page_icon="üéì")

# Kh·ªüi t·∫°o session state
if 'chatbot' not in st.session_state:
    st.session_state.chatbot = None
if 'messages' not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Xin ch√†o! M√¨nh l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ tra c·ª©u S·ªï tay Sinh vi√™n. B·∫°n c·∫ßn t√¨m th√¥ng tin g√¨ h√¥m nay?"}
    ]
if 'history_path' not in st.session_state:
    st.session_state.history_path = "data/cache/chat_history.json"

# H√†m kh·ªüi t·∫°o chatbot
def initialize_chatbot():
    json_path = "data/JSON/SO_TAY_SINH_VIEN.json"
    llm_path = "models/meta/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
    embedding_model = "dangvantuan/vietnamese-document-embedding"
    cache_dir = "data/cache"

    # Container loading
    with st.status("ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng...", expanded=True) as status:
        st.write("üîÑ ƒêang t·∫£i m√¥ h√¨nh ng√¥n ng·ªØ v√† d·ªØ li·ªáu...")
        
        # Kh·ªüi t·∫°o chatbot
        chatbot = SemanticRAGChatbot(
            json_path=json_path,
            llm_path=llm_path,
            embedding_model_name=embedding_model,
            retriever_k=5,
            cache_dir=cache_dir,
            force_rebuild=False
        )
        
        st.write("üìÇ ƒêang kh√¥i ph·ª•c l·ªãch s·ª≠ h·ªôi tho·∫°i...")
        chatbot.load_conversation_history(st.session_state.history_path)
        
        st.session_state.chatbot = chatbot
        status.update(label="S·∫µn s√†ng!", state="complete", expanded=False)

# H√†m x√≥a l·ªãch s·ª≠
def reset_conversation():
    st.session_state.messages = [
        {"role": "assistant", "content": "Xin ch√†o! M√¨nh l√† tr·ª£ l√Ω ·∫£o h·ªó tr·ª£ tra c·ª©u S·ªï tay Sinh vi√™n. B·∫°n c·∫ßn t√¨m th√¥ng tin g√¨ h√¥m nay?"}
    ]
    if st.session_state.chatbot:
        # X√≥a memory trong LangChain
        st.session_state.chatbot.history_store[st.session_state.chatbot.default_session_id].clear()
    st.toast("ƒê√£ b·∫Øt ƒë·∫ßu cu·ªôc tr√≤ chuy·ªán m·ªõi!", icon="üßπ")

# --- UI CH√çNH ---

st.title("üéì Tr·ª£ l√Ω S·ªï tay Sinh vi√™n")
st.caption("H·ªèi ƒë√°p th√¥ng tin quy ch·∫ø, ƒë√†o t·∫°o v√† c√¥ng t√°c sinh vi√™n d·ª±a tr√™n t√†i li·ªáu ch√≠nh th·ª©c.")

# Kh·ªüi t·∫°o chatbot n·∫øu ch∆∞a c√≥
if st.session_state.chatbot is None:
    initialize_chatbot()

# Sidebar c√¥ng c·ª•
with st.sidebar:
    st.header("‚öôÔ∏è C√¥ng c·ª•")
    
    if st.button("L∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i", use_container_width=True):
        if st.session_state.chatbot:
            st.session_state.chatbot.save_conversation_history(st.session_state.history_path)
            st.success("ƒê√£ l∆∞u l·ªãch s·ª≠ th√†nh c√¥ng!")

    if st.button("X√≥a h·ªôi tho·∫°i", type="primary", use_container_width=True):
        reset_conversation()
        st.rerun()

    st.divider()
    st.info("üí° **M·∫πo:** B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ quy ch·∫ø thi, h·ªçc b·ªïng, ƒëi·ªÉm r√®n luy·ªán, ho·∫∑c c√°c th·ªß t·ª•c h√†nh ch√≠nh...")

# Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# X·ª≠ l√Ω input ng∆∞·ªùi d√πng
if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n..."):
    # 1. Hi·ªÉn th·ªã c√¢u h·ªèi ng∆∞·ªùi d√πng
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # 2. X·ª≠ l√Ω tr·∫£ l·ªùi
    if st.session_state.chatbot:
        start_time = time.time()
        
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # Status indicator
            with st.spinner("ƒêang tra c·ª©u t√†i li·ªáu..."):
                try:
                    # G·ªçi stream t·ª´ Chain
                    # Quan tr·ªçng: D√πng ƒë√∫ng session_id m·∫∑c ƒë·ªãnh c·ªßa chatbot class
                    stream_generator = st.session_state.chatbot.qa_chain.stream(
                        {"input": prompt},
                        config={"configurable": {"session_id": st.session_state.chatbot.default_session_id}}
                    )
                    
                    for chunk in stream_generator:
                        if isinstance(chunk, dict) and "answer" in chunk:
                            token = chunk["answer"]
                            full_response += token
                            message_placeholder.markdown(full_response + "‚ñå")
                            
                    # Ho√†n t·∫•t
                    message_placeholder.markdown(full_response)
                    
                except Exception as e:
                    st.error(f"L·ªói h·ªá th·ªëng: {str(e)}")
                    full_response = "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë. Vui l√≤ng th·ª≠ l·∫°i sau."
            
            # Hi·ªÉn th·ªã th·ªùi gian ph·∫£n h·ªìi
            process_time = time.time() - start_time
            st.caption(f"‚è±Ô∏è Ph·∫£n h·ªìi trong {process_time:.2f}s")

        # 3. L∆∞u c√¢u tr·∫£ l·ªùi v√†o session state UI
        st.session_state.messages.append({"role": "assistant", "content": full_response})
