import streamlit as st
import os
from semantic_retrieval import SemanticRAGChatbot
import time
import re

# HÃ m stream text Ä‘Æ°á»£c cáº£i tiáº¿n Ä‘á»ƒ báº£o toÃ n format markdown
def stream_text_improved(text: str, delay: float = 0.02):
    """Stream text theo cÃ¡ch báº£o toÃ n hoÃ n toÃ n Ä‘á»‹nh dáº¡ng markdown"""
    # Chia theo cÃ¡c Ä‘Æ¡n vá»‹ logic: Ä‘oáº¡n vÄƒn, dÃ²ng, cÃ¢u
    # Báº£o toÃ n cÃ¡c kÃ½ tá»± Ä‘áº·c biá»‡t markdown nhÆ° -, *, #, etc.
    
    # Pattern Ä‘á»ƒ chia vÄƒn báº£n thÃ nh cÃ¡c pháº§n cÃ³ Ã½ nghÄ©a
    patterns = [
        r'(\n\n)',  # Äoáº¡n vÄƒn má»›i
        r'(\n(?=[-*â€¢]\s))',  # DÃ²ng báº¯t Ä‘áº§u báº±ng bullet point
        r'(\n(?=\d+\.\s))',  # DÃ²ng báº¯t Ä‘áº§u báº±ng sá»‘
        r'(\n(?=\*\s))',  # DÃ²ng báº¯t Ä‘áº§u báº±ng *
        r'([.!?]\s+)',  # Káº¿t thÃºc cÃ¢u
        r'([,;:]\s+)',  # Dáº¥u pháº©y, cháº¥m pháº©y
    ]
    
    # Chia vÄƒn báº£n thÃ nh cÃ¡c chunk
    chunks = [text]
    for pattern in patterns:
        new_chunks = []
        for chunk in chunks:
            new_chunks.extend(re.split(pattern, chunk))
        chunks = [c for c in new_chunks if c and c.strip()]
    
    # Stream tá»«ng chunk
    for chunk in chunks:
        if chunk.strip():
            yield chunk
            time.sleep(delay)

# HÃ m stream Ä‘Æ¡n giáº£n hÆ¡n - chia theo tá»« nhÆ°ng báº£o toÃ n markdown
def stream_text_simple(text: str, delay: float = 0.03):
    """Stream text theo tá»« nhÆ°ng báº£o toÃ n cÃ¡c kÃ½ tá»± markdown Ä‘áº·c biá»‡t"""
    # TÃ¡ch cÃ¡c dÃ²ng trÆ°á»›c
    lines = text.split('\n')
    
    for i, line in enumerate(lines):
        if line.strip():
            # Náº¿u dÃ²ng báº¯t Ä‘áº§u báº±ng kÃ½ tá»± Ä‘áº·c biá»‡t markdown, giá»¯ nguyÃªn
            if line.strip().startswith(('-', '*', 'â€¢', '+')):
                yield line
                if i < len(lines) - 1:  # KhÃ´ng pháº£i dÃ²ng cuá»‘i
                    yield '\n'
            else:
                # Chia thÃ nh tá»« cho dÃ²ng bÃ¬nh thÆ°á»ng
                words = line.split()
                for j, word in enumerate(words):
                    yield word
                    if j < len(words) - 1:  # KhÃ´ng pháº£i tá»« cuá»‘i
                        yield ' '
                if i < len(lines) - 1:  # KhÃ´ng pháº£i dÃ²ng cuá»‘i
                    yield '\n'
        else:
            # DÃ²ng trá»‘ng
            yield '\n'
        
        time.sleep(delay)

# Khá»Ÿi táº¡o session state
if 'chatbot' not in st.session_state:
    st.session_state.chatbot = None
if 'rag_display_history' not in st.session_state:
    st.session_state.rag_display_history = [{"role": "assistant", "content": "Há»i tÃ´i vá» thÃ´ng tin trong sá»• tay sinh viÃªn nhÃ©!"}]
if 'non_rag_display_history' not in st.session_state:
    st.session_state.non_rag_display_history = [{"role": "assistant", "content": "Há»i tÃ´i vá» há»c táº­p hoáº·c cáº§n tÆ° váº¥n gÃ¬ nhÃ©!"}]
if 'mode' not in st.session_state:
    st.session_state.mode = "rag"  # Máº·c Ä‘á»‹nh lÃ  RAG
if 'history_paths' not in st.session_state:
    st.session_state.history_paths = {
        'rag': "data/cache/rag_history.json",
        'non_rag': "data/cache/non_rag_history.json"
    }

# HÃ m khá»Ÿi táº¡o chatbot vá»›i thanh tiáº¿n trÃ¬nh
def initialize_chatbot():
    json_path = "data/JSON/SO_TAY_SINH_VIEN.json"
    llm_path = "models/meta/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf"
    embedding_model = "dangvantuan/vietnamese-document-embedding"
    cache_dir = "data/cache"

    # Táº¡o container cho thanh tiáº¿n trÃ¬nh vÃ  thÃ´ng bÃ¡o
    progress_container = st.empty()
    status_text = st.empty()
    
    with progress_container:
        progress_bar = st.progress(0)
        
    status_text.write("Äang khá»Ÿi táº¡o...")

    # Giáº£ láº­p cÃ¡c bÆ°á»›c khá»Ÿi táº¡o
    progress_bar.progress(10)  # Báº¯t Ä‘áº§u
    time.sleep(0.1)  # Äá»™ trá»… giáº£ láº­p

    # Khá»Ÿi táº¡o chatbot
    chatbot = SemanticRAGChatbot(
        json_path=json_path,
        llm_path=llm_path,
        embedding_model_name=embedding_model,
        retriever_k=5,
        cache_dir=cache_dir,
        force_rebuild=False
    )
    progress_bar.progress(50)  # HoÃ n thÃ nh khá»Ÿi táº¡o chatbot
    time.sleep(0.1)

    chatbot.mode = st.session_state.mode
    st.session_state.chatbot = chatbot
    
    progress_bar.progress(80)  # Chuáº©n bá»‹ táº£i lá»‹ch sá»­
    time.sleep(0.1)

    # Táº£i lá»‹ch sá»­ há»™i thoáº¡i Ä‘á»ƒ contextualize
    chatbot.load_conversation_history(
        st.session_state.history_paths['rag'],
        st.session_state.history_paths['non_rag']
    )
    
    # Khi Ä‘áº¡t 100%, Ä‘á»•i thÃ´ng bÃ¡o thÃ nh "Khá»Ÿi táº¡o hoÃ n táº¥t"
    progress_bar.progress(100)
    status_text.write("Khá»Ÿi táº¡o hoÃ n táº¥t")
    
    # Äá»£i 1 giÃ¢y sau Ä‘Ã³ xÃ³a cáº£ progress bar vÃ  thÃ´ng bÃ¡o
    time.sleep(1)
    progress_container.empty()
    status_text.empty()

# HÃ m Ä‘áº·t láº¡i lá»‹ch sá»­ hiá»ƒn thá»‹
def reset_display_history():
    if st.session_state.mode == "rag":
        st.session_state.rag_display_history = [{"role": "assistant", "content": "Há»i tÃ´i vá» thÃ´ng tin trong sá»• tay sinh viÃªn nhÃ©!"}]
    else:
        st.session_state.non_rag_display_history = [{"role": "assistant", "content": "Há»i tÃ´i vá» há»c táº­p hoáº·c cáº§n tÆ° váº¥n gÃ¬ nhÃ©!"}]
    st.success("ÄÃ£ báº¯t Ä‘áº§u cuá»™c trÃ² chuyá»‡n má»›i!")

# TiÃªu Ä‘á» á»©ng dá»¥ng
st.title("ğŸ¤– RAG-based chatbot")
st.caption("Hoáº¡t Ä‘á»™ng vá»›i hai cháº¿ Ä‘á»™: tra cá»©u sá»• tay sinh viÃªn (RAG) vÃ  tÆ° váº¥n há»c táº­p (non-RAG)")

# Khá»Ÿi táº¡o chatbot náº¿u chÆ°a cÃ³
if st.session_state.chatbot is None:
    initialize_chatbot()

# Thanh bÃªn Ä‘á»ƒ chá»n cháº¿ Ä‘á»™ vÃ  quáº£n lÃ½ lá»‹ch sá»­
with st.sidebar:
    st.header("CÃ i Ä‘áº·t")
    mode_options = ["RAG (Sá»• tay sinh viÃªn)", "Non-RAG (TÆ° váº¥n há»c táº­p)"]
    selected_mode = st.selectbox(
        "Chá»n cháº¿ Ä‘á»™ trÃ² chuyá»‡n:",
        mode_options,
        index=0 if st.session_state.mode == "rag" else 1
    )
    
    # Cáº­p nháº­t cháº¿ Ä‘á»™
    new_mode = "rag" if selected_mode == "RAG (Sá»• tay sinh viÃªn)" else "non_rag"
    if new_mode != st.session_state.mode:
        st.session_state.mode = new_mode
        st.session_state.chatbot.mode = new_mode
        st.write(f"ÄÃ£ chuyá»ƒn sang cháº¿ Ä‘á»™ {selected_mode}.")

    # NÃºt cuá»™c trÃ² chuyá»‡n má»›i
    if st.button("Cuá»™c trÃ² chuyá»‡n má»›i"):
        reset_display_history()

    # NÃºt lÆ°u lá»‹ch sá»­
    if st.button("LÆ°u lá»‹ch sá»­ há»™i thoáº¡i"):
        st.session_state.chatbot.rag_history = st.session_state.rag_display_history
        st.session_state.chatbot.non_rag_history = st.session_state.non_rag_display_history
        st.session_state.chatbot.save_conversation_history(
            st.session_state.history_paths['rag'],
            st.session_state.history_paths['non_rag']
        )
        st.success("ÄÃ£ lÆ°u lá»‹ch sá»­ há»™i thoáº¡i!")

# Hiá»ƒn thá»‹ lá»‹ch sá»­ trÃ² chuyá»‡n
history = st.session_state.rag_display_history if st.session_state.mode == "rag" else st.session_state.non_rag_display_history
for msg in history:
    st.chat_message(msg["role"]).write(msg["content"])

# Ã” nháº­p cÃ¢u há»i
if prompt := st.chat_input("Nháº­p cÃ¢u há»i cá»§a báº¡n..."):
    start_time = time.time()
    
    st.chat_message("user").write(prompt)
    
    # Stream response vá»›i hiá»‡u á»©ng real-time
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        stream_started = False
        stream_completed = False
        
        # Khá»Ÿi táº¡o status sau khi Ä‘Ã£ setup chat message
        with st.status("Äang tÃ¬m thÃ´ng tin...", expanded=False) as status:
            try:
                # ThÃªm delay Ä‘á»ƒ ngÆ°á»i dÃ¹ng tháº¥y Ä‘Æ°á»£c status Ä‘áº§u tiÃªn
                time.sleep(0.5)
                status.update(label="Äang táº¡o pháº£n há»“i...", state="running")
                
                # Táº¡o stream generator
                if st.session_state.mode == "rag":
                    stream_generator = st.session_state.chatbot.qa_chain.stream(
                        {"input": prompt},
                        config={"configurable": {"session_id": "default"}}
                    )
                else:  # non_rag mode
                    stream_generator = st.session_state.chatbot.academic_support_chain.stream(
                        {"input": prompt},
                        config={"configurable": {"session_id": "default"}}
                    )
                
                # ThÃªm delay trÆ°á»›c khi báº¯t Ä‘áº§u stream
                time.sleep(0.75)
                status.update(label="Äang pháº£n há»“i...", state="running")
                
                # Xá»­ lÃ½ stream chunks vÃ  hiá»ƒn thá»‹ real-time
                chunk_count = 0
                for chunk in stream_generator:
                    chunk_count += 1
                    if not stream_started:
                        stream_started = True
                        # Chá»‰ cáº­p nháº­t status náº¿u thá»±c sá»± cÃ³ content
                        if chunk_count == 1:
                            status.update(label="Äang táº¡o pháº£n há»“i...", state="running")
                    
                    chunk_text = ""
                    
                    if st.session_state.mode == "rag":
                        # Xá»­ lÃ½ chunk cho RAG mode
                        if isinstance(chunk, dict) and "answer" in chunk:
                            chunk_text = chunk["answer"]
                    else:
                        # Xá»­ lÃ½ chunk cho non-RAG mode
                        if hasattr(chunk, 'content') and chunk.content:
                            chunk_text = chunk.content
                        elif isinstance(chunk, dict) and "content" in chunk and chunk["content"]:
                            chunk_text = chunk["content"]
                        elif isinstance(chunk, str) and chunk:
                            chunk_text = chunk
                    
                    # Cáº­p nháº­t response vÃ  hiá»ƒn thá»‹ ngay láº­p tá»©c
                    if chunk_text:
                        full_response += chunk_text
                        # Hiá»ƒn thá»‹ vá»›i cursor Ä‘á»ƒ táº¡o hiá»‡u á»©ng typing
                        message_placeholder.markdown(full_response)
                        time.sleep(0.01)  # Delay nhá» Ä‘á»ƒ táº¡o hiá»‡u á»©ng mÆ°á»£t
                
                # ÄÃ¡nh dáº¥u stream Ä‘Ã£ hoÃ n thÃ nh
                stream_completed = True
                
                # XÃ³a cursor vÃ  hiá»ƒn thá»‹ káº¿t quáº£ cuá»‘i cÃ¹ng
                message_placeholder.markdown(full_response + "|")
                
                # Chá»‰ cáº­p nháº­t status thÃ nh complete khi stream thá»±c sá»± hoÃ n thÃ nh
                if stream_completed and full_response.strip():
                    status.update(label="HoÃ n táº¥t", state="complete")
                else:
                    status.update(label="KhÃ´ng cÃ³ pháº£n há»“i", state="error")
                
            except Exception as e:
                message_placeholder.error(f"CÃ³ lá»—i xáº£y ra: {str(e)}")
                full_response = "Xin lá»—i, cÃ³ lá»—i xáº£y ra khi xá»­ lÃ½ cÃ¢u há»i cá»§a báº¡n."
                status.update(label="Lá»—i xá»­ lÃ½", state="error")
        
        end_time = time.time()
        total_time = end_time - start_time
        st.caption(f"{total_time:.2f}s")
    
    # Cáº­p nháº­t lá»‹ch sá»­ chá»‰ khi cÃ³ pháº£n há»“i há»£p lá»‡
    if full_response.strip():
        if st.session_state.mode == "rag":
            st.session_state.rag_display_history.append({"role": "user", "content": prompt})
            st.session_state.rag_display_history.append({"role": "assistant", "content": full_response})
            st.session_state.chatbot.rag_history.append({"role": "user", "content": prompt})
            st.session_state.chatbot.rag_history.append({"role": "assistant", "content": full_response})
        else:
            st.session_state.non_rag_display_history.append({"role": "user", "content": prompt})
            st.session_state.non_rag_display_history.append({"role": "assistant", "content": full_response})
            st.session_state.chatbot.non_rag_history.append({"role": "user", "content": prompt})
            st.session_state.chatbot.non_rag_history.append({"role": "assistant", "content": full_response})